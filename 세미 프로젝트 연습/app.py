import streamlit as st
from PIL import Image
import io
import numpy as np
import wave
import tempfile
import os
import platform
from datetime import datetime
import openai
from typing import List

# LangChain ë° Pydantic ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# --------------------------------------------------------------------------
# --- 0. ì´ˆê¸° ì„¤ì •: í”Œë«í¼ í™•ì¸, API í‚¤ ë¡œë“œ, í´ë” ìƒì„± ---
# --------------------------------------------------------------------------

# í”Œë«í¼ í™•ì¸ (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ ë§ˆì´í¬ ë…¹ìŒ ê¸°ëŠ¥ í™œì„±í™”)
IS_LOCAL = platform.system() != "Linux"
if IS_LOCAL:
    try:
        import sounddevice as sd
    except Exception as e:
        st.error(f"Sounddevice ë¡œë“œ ì‹¤íŒ¨. ë§ˆì´í¬ ì‚¬ìš©ì´ ë¶ˆê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {e}")
        IS_LOCAL = False

# OpenAI API í‚¤ ì„¤ì •
# ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ: ì»´í“¨í„° í™˜ê²½ ë³€ìˆ˜ì— 'OPENAI_API_KEY' ì„¤ì • í•„ìš”
# Streamlit í´ë¼ìš°ë“œ ë°°í¬ ì‹œ: st.secrets["OPENAI_API_KEY"] ì‚¬ìš© ê¶Œì¥
try:
    openai.api_key = os.getenv("OPENAI_API_KEY")
    if openai.api_key is None:
        st.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
except Exception as e:
    st.error(f"API í‚¤ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
os.makedirs("image_storage", exist_ok=True)


# --------------------------------------------------------------------------
# --- 1. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ì •ì˜ (GPT, LangChain, ì˜¤ë””ì˜¤, ì•„ë°”íƒ€ ë“±) ---
# --------------------------------------------------------------------------

# --- GPT & LangChain Functions ---

class AnalysisResult(BaseModel):
    """ìœ¡í•˜ì›ì¹™ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°"""
    is_complete: bool = Field(description="ì¸í„°ë·° ë‚´ìš©ì— ìœ¡í•˜ì›ì¹™ì˜ í•µì‹¬ ìš”ì†Œ(ëˆ„ê°€, ë¬´ì—‡ì„, ì™œ)ê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€")
    missing_elements: List[str] = Field(description="ëˆ„ë½ëœ ìœ¡í•˜ì›ì¹™ ìš”ì†Œ ëª©ë¡ (í•œêµ­ì–´). ì˜ˆ: ['ì–¸ì œ', 'ì™œ']")
    summary: str = Field(description="í˜„ì¬ê¹Œì§€ íŒŒì•…ëœ ì¸í„°ë·° ë‚´ìš©ì˜ ê°„ëµí•œ ìš”ì•½")

def analyze_transcript_for_completeness(transcript: str) -> AnalysisResult:
    """ì¸í„°ë·° ë‚´ìš©ì„ 'ì°½ì˜ì  ì¶”ë¡  ì—†ì´' ì—„ê²©í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì™„ì „ì„± ì—¬ë¶€ë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
    parser = PydanticOutputParser(pydantic_object=AnalysisResult)
    template = """
    You are a meticulous analyst. Your task is to strictly analyze an interview transcript based on the 5W1H principle (ìœ¡í•˜ì›ì¹™) and determine if it contains enough information.
    A story is considered "complete" ONLY if it clearly contains the core elements: Who, What, and Why.
    CRITICAL INSTRUCTION: Do NOT infer or create any information that is not explicitly present in the transcript.
    Transcript:
    ---
    {transcript}
    ---
    Based *only* on the transcript provided, provide your analysis in the following JSON format.
    {format_instructions}
    """
    prompt = PromptTemplate(template=template, input_variables=["transcript"], partial_variables={"format_instructions": parser.get_format_instructions()})
    chain = LLMChain(llm=llm, prompt=prompt)
    try:
        output = chain.invoke({"transcript": transcript})
        return parser.parse(output['text'])
    except Exception:
        return AnalysisResult(is_complete=True, missing_elements=[], summary="ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

def generate_follow_up_question(summary: str, missing_elements: List[str]) -> str:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ í•  ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
    missing_elements_str = ', '.join([f"'{el}'" for el in missing_elements])
    template = """You are a friendly interviewer. A user's story summary is "{summary}". It's missing these elements: {missing_elements_str}.
    Based on this, generate one friendly follow-up question in Korean to get the missing details."""
    prompt = PromptTemplate.from_template(template)
    chain = LLMChain(llm=llm, prompt=prompt)
    try:
        result = chain.invoke({"summary": summary, "missing_elements_str": missing_elements_str})
        return result['text'].strip()
    except Exception as e:
        return f"ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"

def create_final_video_prompt(family_name: str, theme: str, transcript: str) -> str:
    """ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì˜ìƒ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)
    template = """You are a creative video director. Create a detailed, scene-by-scene storyboard prompt in English.
    Family Name: {family_name}
    Theme: {theme}
    Transcript: {transcript}
    Based on all info, generate a rich prompt describing scenes, camera angles, emotions, and style."""
    prompt = PromptTemplate.from_template(template)
    chain = LLMChain(llm=llm, prompt=prompt)
    try:
        result = chain.invoke({"family_name": family_name, "theme": theme, "transcript": transcript})
        return result['text'].strip()
    except Exception as e:
        return f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"


# --- Audio Processing Functions ---
def record_audio(duration_sec=10, fs=16000):
    st.info(f"{duration_sec}ì´ˆê°„ ì¸í„°ë·° ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        audio_data = sd.rec(int(duration_sec * fs), samplerate=fs, channels=1, dtype='int16')
        sd.wait()
        st.success("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return audio_data.flatten()
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ë…¹ìŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"); return None

def numpy_to_wav_bytes(audio_np, fs=16000):
    buffer = io.BytesIO()
    with wave.open(buffer, 'wb') as wf:
        wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(fs)
        wf.writeframes(audio_np.tobytes())
    buffer.seek(0)
    return buffer

def transcribe_audio(audio_input):
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_input.getvalue())
            tmp_path = tmp_file.name
        with open(tmp_path, "rb") as audio_file:
            result = openai.audio.transcriptions.create(model="whisper-1", file=audio_file, response_format="text")
        return str(result)
    except Exception as e:
        st.error(f"ìŒì„± ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return ""
    finally:
        if tmp_path and os.path.exists(tmp_path): os.remove(tmp_path)


# --- DUMMY Functions (ì‹¤ì œ êµ¬í˜„ í•„ìš”) ---
def extract_face(image_pil):
    st.info("DUMMY: ì–¼êµ´ ì¶”ì¶œ ë¡œì§ ì‹¤í–‰ ì¤‘...")
    return image_pil.resize((256, 256))

def generate_avatar(face_image):
    st.info("DUMMY: AI ì•„ë°”íƒ€ ìƒì„± ë¡œì§ ì‹¤í–‰ ì¤‘...")
    return Image.new('RGB', (512, 512), color = 'blue')

def create_video_from_text_and_image(prompt, image_path):
    st.info(f"DUMMY: ì˜ìƒ ìƒì„± ì¤‘...\n- í”„ë¡¬í”„íŠ¸: {prompt[:50]}...\n- ì´ë¯¸ì§€ ê²½ë¡œ: {image_path}")
    st.success("âœ… (ì˜ˆì‹œ) ì˜ìƒ ìƒì„± ì™„ë£Œ!")
    # ì—¬ê¸°ì— ì‹¤ì œ ì˜ìƒ ìƒì„± API(D-ID, RunwayML ë“±) í˜¸ì¶œ ë¡œì§ êµ¬í˜„


# --------------------------------------------------------------------------
# --- 2. Streamlit UI ë° ìƒíƒœ ê´€ë¦¬ ---
# --------------------------------------------------------------------------

st.title("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ê°€ì¡± ì´ì•¼ê¸° AI ì˜ìƒ ë§Œë“¤ê¸°")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
default_states = {
    "step": "select_theme", "family_name": "", "selected_theme": "",
    "saved_image_path": None, "transcript": "", "final_prompt": None,
    "follow_up_question": None
}
for key, value in default_states.items():
    if key not in st.session_state:
        st.session_state[key] = value

# --- ë‹¨ê³„ë³„ UI ë Œë”ë§ ---

# 1ë‹¨ê³„: í…Œë§ˆ ì„ íƒ
if st.session_state.step == "select_theme":
    st.header("1ë‹¨ê³„: ì˜ìƒ í…Œë§ˆ ì •í•˜ê¸°")
    st.session_state.family_name = st.text_input("ê°€ì¡±ì˜ í˜¸ì¹­ì„ ì…ë ¥í•˜ì„¸ìš”", value=st.session_state.family_name, placeholder="ì˜ˆ: ì‚¬ë‘í•˜ëŠ” ìš°ë¦¬ ê°€ì¡±")
    themes = ["ìš°ë¦¬ì˜ í‰ë²”í•˜ì§€ë§Œ ì†Œì¤‘í•œ ì¼ìƒ", "í•¨ê»˜ ë– ë‚¬ë˜ ì¦ê±°ìš´ ì—¬í–‰ì˜ ì¶”ì–µ", "íŠ¹ë³„í•œ ë‚ ì˜ í–‰ë³µí–ˆë˜ ìˆœê°„ë“¤", "ì•„ì´ë“¤ì˜ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ì„±ì¥ ê¸°ë¡", "ë‹¤ì‹œ ë´ë„ ì›ƒìŒì´ ë‚˜ëŠ” ìš°ë¦¬ ê°€ì¡±ì˜ ì¬ë¯¸ìˆëŠ” ìˆœê°„", "ì„œë¡œì—ê²Œ ì „í•˜ëŠ” ì‚¬ë‘ê³¼ ê°ì‚¬ì˜ ë©”ì‹œì§€"]
    st.session_state.selected_theme = st.radio("ì–´ë–¤ í…Œë§ˆì˜ ì˜ìƒì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?", themes, key="theme_radio")
    
    if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ì–¼êµ´ ì‚¬ì§„ ì…ë ¥ â–¶ï¸", type="primary"):
        if not st.session_state.family_name:
            st.warning("ê°€ì¡±ì˜ í˜¸ì¹­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        else:
            st.session_state.step = "capture_avatar"; st.rerun()

# 2ë‹¨ê³„: ì•„ë°”íƒ€ ìƒì„±
elif st.session_state.step == "capture_avatar":
    st.header("2ë‹¨ê³„: ì˜ìƒì— ì‚¬ìš©í•  ì–¼êµ´ ì‚¬ì§„ ì…ë ¥")
    image_pil = None
    if IS_LOCAL:
        tab1, tab2 = st.tabs(["ğŸ“¸ ì¹´ë©”ë¼ ì´¬ì˜", "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ"])
        with tab1:
            image_file = st.camera_input("ì•„ë°”íƒ€ìš© ì‚¬ì§„ì„ ì°ì–´ë³´ì„¸ìš”");
            if image_file: image_pil = Image.open(image_file)
        with tab2:
            uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"]);
            if uploaded_file: image_pil = Image.open(uploaded_file)
    else:
        st.warning("í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œëŠ” ì¹´ë©”ë¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"]);
        if uploaded_file: image_pil = Image.open(uploaded_file)

    if image_pil:
        with st.spinner("ì–¼êµ´ì„ ì¸ì‹í•˜ê³  ì•„ë°”íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
            face_img = extract_face(image_pil)
            avatar_img = generate_avatar(face_img)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"image_storage/avatar_{timestamp}.jpg"
            avatar_img.save(save_path)
            st.session_state.saved_image_path = save_path
        st.success(f"ì•„ë°”íƒ€ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!"); st.image(avatar_img)

    if st.session_state.saved_image_path:
        if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ì¸í„°ë·° ì¤€ë¹„ â–¶ï¸", type="primary"):
            st.session_state.step = "show_questions"; st.rerun()

# 3ë‹¨ê³„: ì¸í„°ë·° ì§ˆë¬¸ í™•ì¸
elif st.session_state.step == "show_questions":
    st.header("3ë‹¨ê³„: ì¸í„°ë·° ì¤€ë¹„")
    st.info(f"**í…Œë§ˆ: {st.session_state.selected_theme}**")
    st.markdown("ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¡±ê³¼ ììœ ë¡­ê²Œ ëŒ€í™”í•˜ë©° ì¸í„°ë·°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.\n\n(ì˜ˆì‹œ ì§ˆë¬¸: `ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ìˆœê°„ì€ ì–¸ì œì˜€ë‚˜ìš”?`, `ê·¸ë•Œ ê¸°ë¶„ì´ ì–´ë• ë‚˜ìš”?`)")
    if st.button("âœ… ì¤€ë¹„ ì™„ë£Œ! ë…¹ìŒ ì‹œì‘í•˜ê¸° â–¶ï¸", type="primary"):
        st.session_state.step = "record_interview"; st.rerun()

# 4ë‹¨ê³„: ì¸í„°ë·° ë…¹ìŒ ë° ëŒ€í™”í˜• ë¶„ì„
elif st.session_state.step == "record_interview":
    st.header("4ë‹¨ê³„: ì¸í„°ë·° ë…¹ìŒ ë° AI ë¶„ì„")
    
    audio_bytes_io = None
    if IS_LOCAL:
        record_duration = st.slider("ë…¹ìŒí•  ì‹œê°„(ì´ˆ)", 10, 180, 30)
        if st.button(f"ğŸ™ï¸ {record_duration}ì´ˆê°„ ì¸í„°ë·° ë…¹ìŒ ì‹œì‘"):
            audio_np = record_audio(duration_sec=record_duration)
            if audio_np is not None: audio_bytes_io = numpy_to_wav_bytes(audio_np)
    
    uploaded_audio_file = st.file_uploader("ë˜ëŠ” ë…¹ìŒëœ ì¸í„°ë·° íŒŒì¼ ì—…ë¡œë“œ", type=["wav", "mp3", "m4a"])
    if uploaded_audio_file: audio_bytes_io = io.BytesIO(uploaded_audio_file.getvalue())

    if audio_bytes_io:
        st.audio(audio_bytes_io)
        with st.spinner("ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
            transcript = transcribe_audio(audio_bytes_io)
            st.session_state.transcript = st.session_state.transcript + "\n" + transcript if st.session_state.transcript else transcript
            st.info("ìŒì„± ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    if st.session_state.transcript and not st.session_state.final_prompt:
        st.text_area("í˜„ì¬ê¹Œì§€ì˜ ì¸í„°ë·° ë‚´ìš©", st.session_state.transcript, height=150)
        with st.spinner("ë‹µë³€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤..."):
            analysis_result = analyze_transcript_for_completeness(st.session_state.transcript)
            if analysis_result.is_complete:
                st.session_state.final_prompt = create_final_video_prompt(st.session_state.family_name, st.session_state.selected_theme, st.session_state.transcript)
            else:
                st.session_state.follow_up_question = generate_follow_up_question(analysis_result.summary, analysis_result.missing_elements)
        st.rerun()

    if st.session_state.follow_up_question:
        st.warning(st.session_state.follow_up_question)
        if st.button("ğŸ¤ ì¶”ê°€ ë‹µë³€ ë…¹ìŒí•˜ê¸°"):
            st.session_state.follow_up_question = None; st.rerun()

    if st.session_state.final_prompt:
        st.success("âœ¨ ëª¨ë“  ì •ë³´ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ìµœì¢… í™•ì¸ â–¶ï¸", type="primary"):
            st.session_state.step = "create_video"; st.rerun()

# 5ë‹¨ê³„: ìµœì¢… í™•ì¸ ë° ì˜ìƒ ìƒì„±
elif st.session_state.step == "create_video":
    st.header("5ë‹¨ê³„: ìµœì¢… í™•ì¸ ë° ì˜ìƒ ìƒì„±")
    if st.session_state.saved_image_path and st.session_state.final_prompt:
        st.info("ì•„ë˜ ì •ë³´ë¡œ ìµœì¢… ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤.")
        st.image(st.session_state.saved_image_path, caption="ğŸ¨ ìµœì¢… ì˜ìƒìš© ì•„ë°”íƒ€ ì´ë¯¸ì§€")
        st.text_area("ğŸ¬ ìµœì¢… ì˜ìƒ AI í”„ë¡¬í”„íŠ¸", st.session_state.final_prompt, height=200)
        if st.button("ğŸï¸ ì´ ë‚´ìš©ìœ¼ë¡œ ì˜ìƒ ë§Œë“¤ê¸°", type="primary"):
            with st.spinner("ì˜ìƒì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                create_video_from_text_and_image(st.session_state.final_prompt, st.session_state.saved_image_path)
    else:
        st.error("ì˜ìƒ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.")

# --- 'ì²˜ìŒìœ¼ë¡œ' ë²„íŠ¼ ---
if st.session_state.step != "select_theme":
    if st.button("ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        for key in default_states.keys():
            st.session_state[key] = default_states[key]
        st.rerun()