import streamlit as st
from PIL import Image
import io
import numpy as np
import wave
import tempfile
import os
import platform
from datetime import datetime
import openai

# --------------------------------------------------------------------------
# --- 1. gpt_summarizer.pyì—ì„œ í•¨ìˆ˜ import ---
# --------------------------------------------------------------------------
try:
    from summarizer.gpt_summarizer import analyze_transcript_for_completeness, create_final_video_prompt
except ImportError:
    st.error("ì˜¤ë¥˜: gpt_summarizer.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. app.pyì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# --------------------------------------------------------------------------
# --- 2. ì´ˆê¸° ì„¤ì • ë° ëª¨ë“  í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
# --------------------------------------------------------------------------

# í”Œë«í¼ í™•ì¸ (ë¡œì»¬ì—ì„œë§Œ ë§ˆì´í¬ ì‚¬ìš©)
IS_LOCAL = platform.system() != "Linux"
if IS_LOCAL:
    try:
        import sounddevice as sd
    except Exception:
        IS_LOCAL = False

# ì´ë¯¸ì§€ ì €ì¥ í´ë” ìƒì„±
os.makedirs("image_storage", exist_ok=True)

# --- ëª¨ë“  í—¬í¼ í•¨ìˆ˜ (Audio, Dummy, etc.) ---

def record_audio(duration_sec=10, fs=16000):
    """ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë§ˆì´í¬ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•©ë‹ˆë‹¤."""
    st.info(f"{duration_sec}ì´ˆê°„ ì¸í„°ë·° ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        audio_data = sd.rec(int(duration_sec * fs), samplerate=fs, channels=1, dtype='int16')
        sd.wait()
        st.success("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return audio_data.flatten()
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ë…¹ìŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"); return None

def numpy_to_wav_bytes(audio_np, fs=16000):
    """Numpy ì˜¤ë””ì˜¤ ë°°ì—´ì„ WAV í˜•ì‹ì˜ BytesIO ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    buffer = io.BytesIO()
    with wave.open(buffer, 'wb') as wf:
        wf.setnchannels(1); wf.setsampwidth(2); wf.setframerate(fs); wf.writeframes(audio_np.tobytes())
    buffer.seek(0)
    return buffer

def transcribe_audio_from_bytes(audio_bytes_io):
    """BytesIO ì˜¤ë””ì˜¤ ê°ì²´ë¥¼ Whisper APIë¡œ ì „ì‚¬í•©ë‹ˆë‹¤."""
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_bytes_io.getvalue())
            tmp_path = tmp_file.name
        with open(tmp_path, "rb") as audio_file:
            transcript = openai.audio.transcriptions.create(model="whisper-1", file=audio_file, response_format="text")
        return str(transcript)
    except Exception as e:
        st.error(f"ìŒì„± ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return ""
    finally:
        if tmp_path and os.path.exists(tmp_path): os.remove(tmp_path)


# --- DUMMY Functions (ì‹¤ì œ ì„œë¹„ìŠ¤ ì—°ë™ì´ í•„ìš”í•œ ë¶€ë¶„) ---
def extract_face(image_pil):
    """DUMMY: ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ë¶€ë¶„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    return image_pil.resize((256, 256))

def generate_avatar(face_image):
    """DUMMY: AI ì•„ë°”íƒ€ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    return Image.new('RGB', (512, 512), color = 'purple')

def create_video_from_text_and_image(prompt, image_path):
    """DUMMY: ìµœì¢… í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ë¡œ ì˜ìƒì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    st.success("âœ… (ì˜ˆì‹œ) ì˜ìƒ ìƒì„± ì™„ë£Œ!");


# --- ì˜ˆì‹œ ì§ˆë¬¸ ëª©ë¡ ---
interview_questions = {
    "ìš°ë¦¬ì˜ í‰ë²”í•˜ì§€ë§Œ ì†Œì¤‘í•œ ì¼ìƒ": ["ê°€ì¡±ê³¼ í•¨ê»˜í•˜ëŠ” ì•„ì¹¨ ì‹ì‚¬ ì‹œê°„ì€ ì–´ë–¤ ëª¨ìŠµì¸ê°€ìš”?", "ìµœê·¼ì— í•¨ê»˜ ì‚°ì±…í•˜ë©° ë‚˜ëˆˆ ì†Œì†Œí•œ ëŒ€í™”ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”.", "ìš°ë¦¬ ê°€ì¡±ë§Œì˜ ìê¸° ì „ ìŠµê´€ì´ë‚˜ ì£¼ë§ì„ ë³´ë‚´ëŠ” íŠ¹ë³„í•œ ë°©ë²•ì´ ìˆë‚˜ìš”?"],
    "í•¨ê»˜ ë– ë‚¬ë˜ ì¦ê±°ìš´ ì—¬í–‰ì˜ ì¶”ì–µ": ["ì§€ê¸ˆê¹Œì§€ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê°€ì¡± ì—¬í–‰ì€ ì–´ë””ì˜€ë‚˜ìš”? ì™œ ê·¸ê³³ì´ íŠ¹ë³„í–ˆë‚˜ìš”?", "ì—¬í–‰ì§€ì—ì„œ ê²ªì—ˆë˜ ê°€ì¥ ì¬ë¯¸ìˆëŠ” ì—í”¼ì†Œë“œë‚˜ ì˜ˆìƒì¹˜ ëª»í•œ ì‚¬ê±´ì´ ìˆì—ˆë‚˜ìš”?", "ì‚¬ì§„ì„ ë‹¤ì‹œ í¼ì³ë³´ê²Œ ë˜ëŠ”, ê°€ì¥ ë§ˆìŒì— ë“œëŠ” ì—¬í–‰ ì‚¬ì§„ì€ ë¬´ì—‡ì¸ê°€ìš”?"],
    "íŠ¹ë³„í•œ ë‚ ì˜ í–‰ë³µí–ˆë˜ ìˆœê°„ë“¤": ["ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ìƒì¼ì´ë‚˜ ê¸°ë…ì¼ì€ ì–¸ì œì˜€ë‚˜ìš”?", "ìš°ë¦¬ ê°€ì¡±ë§Œì´ ê°€ì§„ íŠ¹ë³„í•œ ëª…ì ˆ ì „í†µì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?", "ì„œë¡œì—ê²Œ ì¤¬ë˜ ì„ ë¬¼ ì¤‘ ê°€ì¥ ê°ë™ì ì´ê±°ë‚˜ ì¬ë¯¸ìˆì—ˆë˜ ê²ƒì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?"],
    "ì•„ì´ë“¤ì˜ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ì„±ì¥ ê¸°ë¡": ["ìë…€ê°€ íƒœì–´ë‚¬ì„ ë•Œ, í˜¹ì€ ì²˜ìŒìœ¼ë¡œ 'ì—„ë§ˆ/ì•„ë¹ 'ë¼ê³  ë¶ˆë €ì„ ë•Œì˜ ê¸°ë¶„ì´ ì–´ë• ë‚˜ìš”?", "í•™ì°½ ì‹œì ˆ, ê°€ì¥ ìë‘ìŠ¤ëŸ¬ì› ë˜ ìˆœê°„ì´ë‚˜ í° ë„ì „ì„ í–ˆë˜ ê¸°ì–µì´ ìˆë‚˜ìš”?", "ë¶€ëª¨ë‹˜ê»˜ì„œëŠ” ìë…€ê°€ ì„±ì¥í•˜ëŠ” ëª¨ìŠµì„ ë³´ë©° ì–¸ì œê°€ ê°€ì¥ ëŒ€ê²¬í•˜ê³  ë¿Œë“¯í–ˆë‚˜ìš”?"],
    "ë‹¤ì‹œ ë´ë„ ì›ƒìŒì´ ë‚˜ëŠ” ìš°ë¦¬ ê°€ì¡±ì˜ ì¬ë¯¸ìˆëŠ” ìˆœê°„": ["ìš°ë¦¬ ê°€ì¡± êµ¬ì„±ì›ë“¤ë§Œ ì•„ëŠ” ì„œë¡œì˜ ì¬ë¯¸ìˆëŠ” ìŠµê´€ì´ë‚˜ ë²„ë¦‡ì´ ìˆë‚˜ìš”?", "ìƒê°ë§Œ í•´ë„ ì›ƒìŒì´ ë‚˜ëŠ” ìš°ë¦¬ ê°€ì¡±ì˜ 'í‘ì—­ì‚¬'ë‚˜ ì¬ë¯¸ìˆëŠ” ì‹¤ìˆ˜ë‹´ì´ ìˆë‹¤ë©´?", "ê°€ì¥ ì„±ê³µì ì´ì—ˆë˜ (ë˜ëŠ” ì™„ì „íˆ ì‹¤íŒ¨í–ˆë˜) ê°€ì¡± ì¥ë‚œì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?"],
    "ì„œë¡œì—ê²Œ ì „í•˜ëŠ” ì‚¬ë‘ê³¼ ê°ì‚¬ì˜ ë©”ì‹œì§€": ["ê°€ì¡±ì´ì–´ì„œ ê°€ì¥ í˜ì´ ë˜ì—ˆë˜ ìˆœê°„ì€ ì–¸ì œì˜€ë‚˜ìš”?", "ì„œë¡œì—ê²Œ í‰ì†Œì— ì‘¥ìŠ¤ëŸ¬ì›Œì„œ í•˜ì§€ ëª»í–ˆë˜ ê³ ë§ˆìš´ ë§ˆìŒì„ í‘œí˜„í•´ì£¼ì„¸ìš”.", "ì•ìœ¼ë¡œ ìš°ë¦¬ ê°€ì¡±ì´ í•¨ê»˜ ì´ë£¨ê³  ì‹¶ì€ ê¿ˆì´ë‚˜ ì†Œë§ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?"]
}

# --------------------------------------------------------------------------
# --- 3. Streamlit UI ë° ìƒíƒœ ê´€ë¦¬ ---
# --------------------------------------------------------------------------

st.title("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ê°€ì¡± ì´ì•¼ê¸° AI ì˜ìƒ ë§Œë“¤ê¸°")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
default_states = {
    "step": "select_theme", "family_name": "", "selected_theme": list(interview_questions.keys())[0],
    "saved_image_path": None, "final_prompt": None,
    "qa_list": [], "current_question": "", "transcript": ""
}
for key, value in default_states.items():
    if key not in st.session_state:
        st.session_state[key] = value

# --- ë‹¨ê³„ë³„ UI ë Œë”ë§ ---

if st.session_state.step == "select_theme":
    st.header("1ë‹¨ê³„: ì˜ìƒ í…Œë§ˆ ì •í•˜ê¸°")
    st.session_state.family_name = st.text_input("ê°€ì¡±ì˜ í˜¸ì¹­ì„ ì…ë ¥í•˜ì„¸ìš”", value=st.session_state.family_name, placeholder="ì˜ˆ: ì‚¬ë‘í•˜ëŠ” ìš°ë¦¬ ê°€ì¡±")
    themes = list(interview_questions.keys())
    st.session_state.selected_theme = st.radio("ì–´ë–¤ í…Œë§ˆì˜ ì˜ìƒì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?", themes, index=themes.index(st.session_state.selected_theme))
    
    if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ì–¼êµ´ ì‚¬ì§„ ì…ë ¥ â–¶ï¸", type="primary"):
        if not st.session_state.family_name:
            st.warning("ê°€ì¡±ì˜ í˜¸ì¹­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        else:
            st.session_state.step = "capture_avatar"; st.rerun()

elif st.session_state.step == "capture_avatar":
    st.header("2ë‹¨ê³„: ì˜ìƒì— ì‚¬ìš©í•  ì–¼êµ´ ì‚¬ì§„ ì…ë ¥")
    image_pil = None
    if IS_LOCAL:
        tab1, tab2 = st.tabs(["ğŸ“¸ ì¹´ë©”ë¼ ì´¬ì˜", "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ"])
        with tab1: image_file = st.camera_input("ì•„ë°”íƒ€ìš© ì‚¬ì§„ì„ ì°ì–´ë³´ì„¸ìš”")
        with tab2: uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
    else:
        st.warning("í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œëŠ” ì¹´ë©”ë¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
        image_file = None
    
    if image_file if 'image_file' in locals() and image_file is not None else None: image_pil = Image.open(image_file)
    if uploaded_file: image_pil = Image.open(uploaded_file)

    if image_pil:
        with st.spinner("ì–¼êµ´ì„ ì¸ì‹í•˜ê³  ì•„ë°”íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
            face_img = extract_face(image_pil)
            avatar_img = generate_avatar(face_img)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"image_storage/avatar_{timestamp}.jpg"
            avatar_img.save(save_path)
            st.session_state.saved_image_path = save_path
        st.success(f"ì•„ë°”íƒ€ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!"); st.image(avatar_img, caption="ìƒì„±ëœ AI ì•„ë°”íƒ€")

    if st.session_state.saved_image_path:
        if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ì¸í„°ë·° ì¤€ë¹„ â–¶ï¸", type="primary"):
            st.session_state.step = "show_questions"; st.rerun()

elif st.session_state.step == "show_questions":
    st.header("3ë‹¨ê³„: ì¸í„°ë·° ì¤€ë¹„")
    st.info(f"**ì„ íƒ í…Œë§ˆ:** {st.session_state.selected_theme}")
    st.markdown("ì•„ë˜ ì§ˆë¬¸ë“¤ì„ ë³´ë©° ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ í• ì§€ ììœ ë¡­ê²Œ êµ¬ìƒí•´ë³´ì„¸ìš”.")
    questions = interview_questions.get(st.session_state.selected_theme, [])
    for i, q in enumerate(questions): st.markdown(f"**Q{i+1}.** {q}")
    if st.button("âœ… ì¤€ë¹„ ì™„ë£Œ! ë…¹ìŒ ì‹œì‘í•˜ê¸° â–¶ï¸", type="primary"):
        st.session_state.step = "record_interview"; st.rerun()

elif st.session_state.step == "record_interview":
    st.header("4ë‹¨ê³„: ëŒ€í™”í˜• ì¸í„°ë·° ì§„í–‰")

    with st.expander("ğŸ“– ì„ íƒí•œ í…Œë§ˆì˜ ì˜ˆì‹œ ì§ˆë¬¸ ë‹¤ì‹œë³´ê¸°", expanded=True):
        questions = interview_questions.get(st.session_state.selected_theme, [])
        for q in questions: st.markdown(f"- {q}")
    st.markdown("---")

    if st.session_state.qa_list:
        st.subheader("âœ… ì™„ì„±ëœ ì§ˆë¬¸/ë‹µë³€ ëª©ë¡")
        for i, qa in enumerate(st.session_state.qa_list):
            with st.container(border=True):
                st.markdown(f"**Q{i+1}.** {qa['question']}")
                st.markdown(f"**A{i+1}.** {qa['answer']}")
        st.markdown("---")

    st.subheader("â• ìƒˆë¡œìš´ ì§ˆë¬¸ & ë‹µë³€ ì¶”ê°€í•˜ê¸°")

    if not st.session_state.current_question:
        st.markdown("**1. ë¨¼ì € ì§ˆë¬¸ì„ ë…¹ìŒí•˜ì„¸ìš”.**")
        if IS_LOCAL and st.button("ğŸ™ï¸ ì§ˆë¬¸ ë…¹ìŒí•˜ê¸° (5ì´ˆ)"):
            with st.spinner("ì§ˆë¬¸ì„ ë…¹ìŒí•˜ê³  ë³€í™˜ ì¤‘..."):
                audio_np = record_audio(duration_sec=5)
                if audio_np is not None:
                    wav_bytes = numpy_to_wav_bytes(audio_np)
                    st.session_state.current_question = transcribe_audio_from_bytes(wav_bytes)
                    st.rerun()
    else:
        st.success(f"**ë…¹ìŒëœ ì§ˆë¬¸:** {st.session_state.current_question}")
        st.markdown("**2. ì´ì œ ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë…¹ìŒí•˜ì„¸ìš”.**")

        with st.container(border=True):
            st.markdown("ğŸ’¡ **ë‹µë³€ Tip: í’ë¶€í•œ ì´ì•¼ê¸°ë¥¼ ìœ„í•œ ìœ¡í•˜ì›ì¹™!**")
            st.caption("ë‹µë³€í•˜ì‹¤ ë•Œ ì•„ë˜ ë‚´ìš©ì„ í¬í•¨í•˜ë©´ ë” ìƒìƒí•˜ê³  ê°ë™ì ì¸ ì´ì•¼ê¸°ê°€ ë¼ìš”.")
            st.markdown("- **ëˆ„ê°€ (Who):** ì´ì•¼ê¸°ì˜ ì£¼ì¸ê³µì€ ëˆ„êµ¬ì¸ê°€ìš”?\n- **ì–¸ì œ (When):** ê·¸ ì¼ì€ ì–¸ì œ ìˆì—ˆë‚˜ìš”?\n- **ì–´ë””ì„œ (Where):** ì–´ë–¤ ì¥ì†Œì˜€ë‚˜ìš”?\n- **ë¬´ì—‡ì„ (What):** ì–´ë–¤ íŠ¹ë³„í•œ ì‚¬ê±´ì´ ìˆì—ˆë‚˜ìš”?\n- **ì™œ (Why):** ê·¸ ìˆœê°„ì´ ì™œ ì¤‘ìš”í•˜ê³  íŠ¹ë³„í–ˆë‚˜ìš”?\n- **ì–´ë–»ê²Œ (How):** ë‹¹ì‹œì˜ ë¶„ìœ„ê¸°ë‚˜ ê°ì •ì€ ì–´ë• ë‚˜ìš”?")
        
        record_duration = st.slider("ë‹µë³€ ë…¹ìŒ ì‹œê°„(ì´ˆ)", 10, 180, 30, key="answer_duration")
        if IS_LOCAL and st.button(f"ğŸ¤ ë‹µë³€ ë…¹ìŒí•˜ê¸° ({record_duration}ì´ˆ)"):
            with st.spinner("ë‹µë³€ì„ ë…¹ìŒí•˜ê³  ë³€í™˜ ì¤‘..."):
                audio_np = record_audio(duration_sec=record_duration)
                if audio_np is not None:
                    wav_bytes = numpy_to_wav_bytes(audio_np)
                    answer = transcribe_audio_from_bytes(wav_bytes)
                    if answer:
                        st.session_state.qa_list.append({"question": st.session_state.current_question, "answer": answer})
                        st.session_state.current_question = ""
                        st.rerun()
    st.markdown("---")

    if st.session_state.qa_list:
        if st.button("âœ… ì¸í„°ë·° ì™„ë£Œ ë° ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("ì „ì²´ ì¸í„°ë·° ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                full_transcript = "\n\n".join([f"Q: {qa['question']}\nA: {qa['answer']}" for qa in st.session_state.qa_list])
                st.session_state.transcript = full_transcript
                analysis_result = analyze_transcript_for_completeness(full_transcript)
                if analysis_result.is_complete:
                    final_prompt = create_final_video_prompt(st.session_state.family_name, st.session_state.selected_theme, full_transcript)
                    st.session_state.final_prompt = final_prompt
                else:
                    st.warning("âš ï¸ ì´ì•¼ê¸°ì˜ í•µì‹¬ ìš”ì†Œ(ëˆ„ê°€, ë¬´ì—‡ì„, ì™œ)ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\n\nì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë” ì¶”ê°€í•˜ì—¬ ì´ì•¼ê¸°ë¥¼ êµ¬ì²´í™”í•´ì£¼ì„¸ìš”.")
            if st.session_state.final_prompt:
                st.success("ì¶©ë¶„í•œ ë‚´ìš©ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤! ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                st.rerun()
    
    if st.session_state.final_prompt:
        st.success("âœ¨ AI í”„ë¡¬í”„íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.text_area("ìƒì„±ëœ ìµœì¢… AI í”„ë¡¬í”„íŠ¸", st.session_state.final_prompt, height=200)
        if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ: ìµœì¢… í™•ì¸ â–¶ï¸"):
            st.session_state.step = "create_video"; st.rerun()

elif st.session_state.step == "create_video":
    st.header("5ë‹¨ê³„: ìµœì¢… í™•ì¸ ë° ì˜ìƒ ìƒì„±")
    if st.session_state.saved_image_path and st.session_state.final_prompt:
        st.info("ì•„ë˜ ì •ë³´ë¡œ ìµœì¢… ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤.")
        st.image(st.session_state.saved_image_path, caption="ğŸ¨ ìµœì¢… ì˜ìƒìš© ì•„ë°”íƒ€ ì´ë¯¸ì§€")
        st.text_area("ğŸ¬ ìµœì¢… ì˜ìƒ AI í”„ë¡¬í”„íŠ¸", st.session_state.final_prompt, height=200)
        if st.button("ğŸï¸ ì´ ë‚´ìš©ìœ¼ë¡œ ì˜ìƒ ë§Œë“¤ê¸°", type="primary"):
            with st.spinner("ì˜ìƒì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                create_video_from_text_and_image(st.session_state.final_prompt, st.session_state.saved_image_path)
    else:
        st.error("ì˜ìƒ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.")

if st.session_state.step != "select_theme":
    if st.button("ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        for key in default_states.keys():
            st.session_state[key] = default_states[key]
        st.rerun()

