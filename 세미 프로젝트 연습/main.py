import os
import io
from PIL import Image
from stt.whs_st import load_model, record_audio, numpy_to_wav_bytes, transcribe_audio
from summarizer.gpt_summarizer import summarize_text, generate_video_script
from camera.face_capture import extract_face
from avatar_create.avatar_generator import generate_avatar  # ì•„ë°”íƒ€ ìƒì„± í•¨ìˆ˜ import
from video.Text_To_video import make_video_from_text, save_wav_file
from avatar_create.avatar_batch_processor import extract_faces_with_positions, generate_enhanced_faces, replace_faces_on_image

# --- ì„¤ì • ---
AUDIO_FILE = "audio/recorded.wav"
ORIGINAL_IMAGE_FILE = "camera/sample.jpg"
FACE_IMAGE_FILE = "camera/face_extracted.jpg"
AVATAR_IMAGE_FILE = "camera/avatar.jpg"  # ì•„ë°”íƒ€ ì´ë¯¸ì§€ ê²½ë¡œ
VIDEO_OUTPUT_PATH = "output/output.mp4"

# --- ë””ë ‰í† ë¦¬ ìƒì„± ---
os.makedirs("audio", exist_ok=True)
os.makedirs("camera", exist_ok=True)
os.makedirs("output", exist_ok=True)

# --- 1. ì˜¤ë””ì˜¤ ë…¹ìŒ ë° í…ìŠ¤íŠ¸ ì „ì‚¬ ---
print("1. ì˜¤ë””ì˜¤ ë…¹ìŒ ë° ì „ì‚¬ ì‹œì‘...")
model = load_model()
audio_np = record_audio(duration_sec=5)
wav_bytes = numpy_to_wav_bytes(audio_np)
wav_bytes.seek(0)  # í¬ì¸í„° ìœ„ì¹˜ ì´ˆê¸°í™”

# WAV ì €ì¥
with open(AUDIO_FILE, "wb") as f:
    f.write(wav_bytes.read())
print(f"ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {AUDIO_FILE}")

# ì „ì‚¬
with open(AUDIO_FILE, "rb") as f:
    transcribe_result = transcribe_audio(model, io.BytesIO(f.read()), return_all=True)

transcription = transcribe_result["transcript"]
summary = transcribe_result["summary"]
script = transcribe_result["video_script"]

print("\nğŸ“ ì „ì‚¬ ê²°ê³¼:")
print(transcription)
print("\nğŸ“‘ ìš”ì•½:")
print(summary)
print("\nğŸ¬ ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸:")
print(script)

# --- 2. ì–¼êµ´ ì´ë¯¸ì§€ ì²˜ë¦¬ ---
print("\n2. ì–¼êµ´ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘...")
if not os.path.exists(ORIGINAL_IMAGE_FILE):
    raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ORIGINAL_IMAGE_FILE}")

original_img = Image.open(ORIGINAL_IMAGE_FILE)
face_img = extract_face(original_img, save_path=FACE_IMAGE_FILE)

if face_img is None:
    print("âš ï¸ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    image_path = ORIGINAL_IMAGE_FILE
else:
    print(f"âœ… ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {FACE_IMAGE_FILE}")

    # --- 2.5 ì•„ë°”íƒ€ ì´ë¯¸ì§€ ìƒì„± ---
    print("\n2.5 ì•„ë°”íƒ€ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘...")
    avatar_img = generate_avatar(face_img)
    avatar_img.save(AVATAR_IMAGE_FILE)
    print(f"âœ… ì•„ë°”íƒ€ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {AVATAR_IMAGE_FILE}")

    image_path = AVATAR_IMAGE_FILE  # ì˜ìƒ ìƒì„±ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ ê²½ë¡œ ê°±ì‹ 

# --- 3. ì˜ìƒ ìƒì„± ---
print("\n3. ì˜ìƒ ìƒì„± ì‹œì‘...")
# make_video_from_text í•¨ìˆ˜ê°€ image_path íŒŒë¼ë¯¸í„°ë¥¼ ë°›ë„ë¡ ìˆ˜ì •ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
make_video_from_text(script, output_path=VIDEO_OUTPUT_PATH, image_path=image_path)
print(f"\nâœ… ì˜ìƒ ìƒì„± ì™„ë£Œ: {VIDEO_OUTPUT_PATH}")
